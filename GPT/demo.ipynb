{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(2, 4)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## NN.Embedding\n",
    "vocab_size = 2 \n",
    "embed_dims= 4\n",
    "nn.Embedding(vocab_size, embedding_dim=embed_dims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0280, -0.1462, -0.1974, -0.7201],\n",
      "         [ 0.9385, -0.0535,  0.1047, -1.4124],\n",
      "         [-0.8077, -1.3785, -1.2666,  1.0047],\n",
      "         [-0.3406,  0.5757,  0.5708, -1.1245],\n",
      "         [ 0.3573, -2.5372,  0.6559, -0.5169],\n",
      "         [ 0.3628,  0.4259,  1.2958,  0.5087],\n",
      "         [-0.2399,  1.4832, -0.8201, -0.0881],\n",
      "         [ 0.7397, -0.7980, -0.2014,  1.3863],\n",
      "         [ 0.1658,  0.4730,  0.7593, -0.8754]],\n",
      "\n",
      "        [[-0.8077, -1.3785, -1.2666,  1.0047],\n",
      "         [-2.4329, -1.0504,  0.7781, -0.9709],\n",
      "         [ 0.9385, -0.0535,  0.1047, -1.4124],\n",
      "         [ 0.7397, -0.7980, -0.2014,  1.3863],\n",
      "         [-0.3406,  0.5757,  0.5708, -1.1245],\n",
      "         [ 0.3628,  0.4259,  1.2958,  0.5087],\n",
      "         [-0.2399,  1.4832, -0.8201, -0.0881],\n",
      "         [ 0.7397, -0.7980, -0.2014,  1.3863],\n",
      "         [ 0.1658,  0.4730,  0.7593, -0.8754]]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# an Embedding module containing 23 tensors (from Zero to 22) of size 4\n",
    "embedding = nn.Embedding(num_embeddings= 23, embedding_dim= 4)\n",
    "# a batch of 2 samples of 4 indices each\n",
    "input = torch.LongTensor([[1, 2, 4, 5, 6,7,8,9,22], [4, 3, 2, 9,5,7,8,9,22]])\n",
    "print(embedding(input))\n",
    "# 1,2,4,5,3,9,6, 7, 8,9,\n",
    "# 8 +1\n",
    "## here (Input Value/tensor element should be less than 23, as we passed num_embedding=23)\n",
    "## It'll accept number till 22 only, else throw Error\n",
    "\n",
    "## Embedding Dimensions\n",
    "# As here Embedding Dimension set as 4: \n",
    "# So Each number will be represented by 4 dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "print(embedding(input).shape)\n",
    "# torch.Size([2, 9, 4])\n",
    "# Batch Size, rows, cols \n",
    "# or Batch Size, no_elements, EmbedDims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2311, -0.1467,  1.6282],\n",
       "         [ 0.4952, -0.2592,  0.2595],\n",
       "         [-0.2311, -0.1467,  1.6282],\n",
       "         [ 0.1102, -0.2073,  0.9205]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example with padding_idx\n",
    "#  specified, the entries at padding_idx do not contribute to the gradient; therefore, the embedding vector at padding_idx is not updated during training, i.e. it remains as a fixed “pad”. For a newly constructed Embedding, the embedding vector at padding_idx will default to all zeros, but can be updated to another value to be used as the padding vector.\n",
    "embedding = nn.Embedding(10, 3, padding_idx=0)\n",
    "input = torch.LongTensor([[1, 2, 1, 5]])\n",
    "embedding(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.LongTensor([[1, 2, 4, 5, 6,7,8,9,22,24], \n",
    "                  [4, 3, 2, 9,5,7,8,9,22,28]])\n",
    "tens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 10)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, seq_length =tens.shape\n",
    "batch_size, seq_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## torch.arange()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,   1,   2,   3,   4,   5,   6,   7,   8,   9,  10,  11,  12,  13,\n",
       "         14,  15,  16,  17,  18,  19,  20,  21,  22,  23,  24,  25,  26,  27,\n",
       "         28,  29,  30,  31,  32,  33,  34,  35,  36,  37,  38,  39,  40,  41,\n",
       "         42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  52,  53,  54,  55,\n",
       "         56,  57,  58,  59,  60,  61,  62,  63,  64,  65,  66,  67,  68,  69,\n",
       "         70,  71,  72,  73,  74,  75,  76,  77,  78,  79,  80,  81,  82,  83,\n",
       "         84,  85,  86,  87,  88,  89,  90,  91,  92,  93,  94,  95,  96,  97,\n",
       "         98,  99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111,\n",
       "        112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "        126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139,\n",
       "        140, 141, 142, 143, 144, 145, 146, 147, 148, 149, 150, 151, 152, 153,\n",
       "        154, 155, 156, 157, 158, 159, 160, 161, 162, 163, 164, 165, 166, 167,\n",
       "        168, 169, 170, 171, 172, 173, 174, 175, 176, 177, 178, 179, 180, 181,\n",
       "        182, 183, 184, 185, 186, 187, 188, 189, 190, 191, 192, 193, 194, 195,\n",
       "        196, 197, 198, 199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209,\n",
       "        210, 211, 212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223,\n",
       "        224, 225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
       "        238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250, 251,\n",
       "        252, 253, 254, 255])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.arange(256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens position:  tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "positional Embeddings Shape>>  torch.Size([10, 4])\n",
      "positional Embeddings >>  tensor([[-1.9608, -1.4489, -1.1177,  1.2926],\n",
      "        [-0.9589, -0.9396,  1.9117, -1.1692],\n",
      "        [-0.0338, -0.8293,  0.9690, -0.1280],\n",
      "        [ 1.0294, -0.4790,  0.9730, -1.5220],\n",
      "        [-1.9138, -0.1396, -0.1988,  0.6087],\n",
      "        [-0.2801,  0.7829, -1.2065,  1.1446],\n",
      "        [-0.2788,  0.1322,  0.8448,  0.6094],\n",
      "        [-0.3390, -0.5185, -1.0499,  0.5883],\n",
      "        [ 0.4952,  0.3570, -0.2264,  0.0921],\n",
      "        [ 1.1720,  0.4919, -1.1811, -0.2674]], grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "pos_embedding= nn.Embedding(256, 4)\n",
    "seq_length= 10\n",
    "positional_embedding= pos_embedding(torch.arange(10))\n",
    "print(\"Tokens position: \",torch.arange(10))\n",
    "print(\"positional Embeddings Shape>> \", positional_embedding.shape)\n",
    "print(\"positional Embeddings >> \", positional_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding Tokens :  tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "token Embeddings Shape>>  torch.Size([10, 4])\n",
      "Token Embeddings >>  tensor([[ 1.1816e+00,  4.7038e-01, -5.1939e-02,  1.1171e+00],\n",
      "        [ 7.6613e-01,  2.2593e-01,  2.8409e-01,  1.6862e+00],\n",
      "        [-5.6263e-01, -2.4929e-01,  1.2803e-01,  1.7724e+00],\n",
      "        [-1.5313e+00, -3.1009e-01, -2.2577e+00,  1.6232e+00],\n",
      "        [-6.2501e-02,  4.3446e-01,  1.0153e+00,  1.4387e+00],\n",
      "        [ 8.0280e-01, -3.3533e-01,  9.9792e-01, -5.6364e-01],\n",
      "        [ 5.6120e-01, -3.8867e-01, -8.0653e-01,  1.5267e-01],\n",
      "        [-6.7787e-01, -1.7172e+00,  1.5969e-03,  2.8122e-01],\n",
      "        [ 1.5161e+00, -2.1682e-01,  3.7225e-01, -7.7389e-01],\n",
      "        [ 1.3797e-01,  1.4421e+00,  1.6830e+00,  1.1525e-01]],\n",
      "       grad_fn=<EmbeddingBackward0>)\n"
     ]
    }
   ],
   "source": [
    "tok_emb= nn.Embedding(256, 4)\n",
    "seq_length= 10\n",
    "token_embedding= tok_emb(torch.arange(10))\n",
    "print(\"Embedding Tokens : \",torch.arange(10))\n",
    "print(\"token Embeddings Shape>> \", token_embedding.shape)\n",
    "print(\"Token Embeddings >> \", token_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.7792, -0.9785, -1.1697,  2.4096],\n",
       "        [-0.1928, -0.7137,  2.1958,  0.5171],\n",
       "        [-0.5964, -1.0786,  1.0971,  1.6444],\n",
       "        [-0.5019, -0.7891, -1.2847,  0.1012],\n",
       "        [-1.9763,  0.2949,  0.8165,  2.0474],\n",
       "        [ 0.5227,  0.4476, -0.2085,  0.5810],\n",
       "        [ 0.2824, -0.2565,  0.0383,  0.7621],\n",
       "        [-1.0169, -2.2357, -1.0483,  0.8695],\n",
       "        [ 2.0113,  0.1402,  0.1459, -0.6818],\n",
       "        [ 1.3099,  1.9340,  0.5018, -0.1522]], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_embedding + positional_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN.linear()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```markdown\n",
    "# `torch.nn.Linear` in PyTorch\n",
    "\n",
    "In PyTorch, `torch.nn.Linear` is a commonly used layer that applies a **linear transformation** (also known as a **fully connected** or **dense** layer) to the input data. It is part of the `torch.nn` module, which contains various neural network layers.\n",
    "\n",
    "## Definition\n",
    "```python\n",
    "torch.nn.Linear(in_features, out_features, bias=True)\n",
    "```\n",
    "\n",
    "### Parameters\n",
    "- **`in_features`**: The size of each input sample (number of input features).\n",
    "- **`out_features`**: The size of each output sample (number of output features).\n",
    "- **`bias`** (optional): If `True`, includes a learnable bias term. By default, it is set to `True`.\n",
    "\n",
    "## Mathematical Operation\n",
    "For an input vector \\( x \\) and weight matrix \\( W \\) with an optional bias \\( b \\), the output \\( y \\) of the linear layer is computed as:\n",
    "\n",
    "\\[\n",
    "y = xW^T + b\n",
    "\\]\n",
    "\n",
    "### Notation\n",
    "- **\\( x \\)**: Input tensor of shape \\((N, \\text{in\\_features})\\)\n",
    "- **\\( W \\)**: Weight tensor of shape \\((\\text{out\\_features}, \\text{in\\_features})\\)\n",
    "- **\\( b \\)**: Bias tensor of shape \\((\\text{out\\_features})\\)\n",
    "\n",
    "This linear transformation layer is a fundamental building block for creating fully connected layers in neural network architectures.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_model = 2\n",
    "qkv = nn.Linear(d_model, d_model*3, bias=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 30])\n"
     ]
    }
   ],
   "source": [
    "m = nn.Linear(20, 30)\n",
    "input = torch.randn(128, 20)\n",
    "output = m(input)\n",
    "print(output.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 0.4407, -0.0099,  0.0658, -0.2887]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Define a simple neural network with one hidden layer\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.hidden = nn.Linear(4, 3)  # 4 input features, 3 hidden units\n",
    "        self.output = nn.Linear(3, 4)  # 3 hidden units, 1 output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden(x))\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = SimpleNN()\n",
    "\n",
    "# Define a sample input\n",
    "input_data = torch.tensor([[0.5, 0.3, 0.2, 0.1]])\n",
    "\n",
    "# Perform a forward pass\n",
    "output_data = model(input_data)\n",
    "\n",
    "print(\"Output:\", output_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output: tensor([[ 0.4407, -0.0099,  0.0658, -0.2887],\n",
      "        [ 0.4407, -0.0099,  0.0658, -0.2887],\n",
      "        [ 0.4407, -0.0099,  0.0658, -0.2887],\n",
      "        [ 0.4407, -0.0099,  0.0658, -0.2887]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Define a sample input\n",
    "input_data = torch.tensor([[0.5, 0.3, 0.2, 0.1], \n",
    "                           [0.5, 0.3, 0.2, 0.1],\n",
    "                           [0.5, 0.3, 0.2, 0.1],\n",
    "                           [0.5, 0.3, 0.2, 0.1]])\n",
    "\n",
    "# Perform a forward pass\n",
    "output_data = model(input_data)\n",
    "\n",
    "print(\"Output:\", output_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QKV in transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Shapes of \\( Q \\), \\( K \\), and \\( V \\) in Transformer Architecture\n",
    "\n",
    "In Transformer architectures, the shapes of the **Query (Q)**, **Key (K)**, and **Value (V)** tensors are determined by the following factors:\n",
    "\n",
    "1. **Input Embedding Dimension** (`embed_dim`): The size of the input embeddings, denoted as \\( d_{\\text{model}} \\).\n",
    "2. **Number of Attention Heads** (`num_heads`): The number of parallel attention heads, denoted as \\( h \\).\n",
    "3. **Head Dimension** (`head_dim`): The size of each attention head, calculated as:\n",
    "\n",
    "   \\[\n",
    "   \\text{head\\_dim} = \\frac{\\text{embed\\_dim}}{\\text{num\\_heads}}\n",
    "   \\]\n",
    "\n",
    "## Shapes of \\( Q \\), \\( K \\), and \\( V \\)\n",
    "\n",
    "Given an input tensor with shape:\n",
    "\n",
    "\\[\n",
    "(\\text{batch\\_size}, \\text{seq\\_length}, \\text{embed\\_dim})\n",
    "\\]\n",
    "\n",
    "The shapes of the Query (Q), Key (K), and Value (V) tensors after applying linear transformations are:\n",
    "\n",
    "\\[\n",
    "Q, K, V: \\; (\\text{batch\\_size}, \\text{seq\\_length}, \\text{embed\\_dim})\n",
    "\\]\n",
    "\n",
    "### After Reshaping and Splitting into Heads\n",
    "\n",
    "For multi-head attention, we reshape \\( Q \\), \\( K \\), and \\( V \\) to include the number of heads:\n",
    "\n",
    "\\[\n",
    "(\\text{batch\\_size}, \\text{num\\_heads}, \\text{seq\\_length}, \\text{head\\_dim})\n",
    "\\]\n",
    "\n",
    "where:\n",
    "\n",
    "\\[\n",
    "\\text{head\\_dim} = \\frac{\\text{embed\\_dim}}{\\text{num\\_heads}}\n",
    "\\]\n",
    "\n",
    "### Concatenated Output Shape\n",
    "\n",
    "After computing attention scores, the outputs from all heads are concatenated along the embedding dimension:\n",
    "\n",
    "\\[\n",
    "\\text{Output Shape}: \\; (\\text{batch\\_size}, \\text{seq\\_length}, \\text{embed\\_dim})\n",
    "\\]\n",
    "\n",
    "## Example Calculation\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- \\( \\text{embed\\_dim} = 512 \\)\n",
    "- \\( \\text{num\\_heads} = 8 \\)\n",
    "\n",
    "Then:\n",
    "\n",
    "1. **Head Dimension**:\n",
    "\n",
    "   \\[\n",
    "   \\text{head\\_dim} = \\frac{512}{8} = 64\n",
    "   \\]\n",
    "\n",
    "2. **Shape of \\( Q \\), \\( K \\), and \\( V \\) After Reshaping**:\n",
    "\n",
    "   \\[\n",
    "   (\\text{batch\\_size}, 8, \\text{seq\\_length}, 64)\n",
    "   \\]\n",
    "\n",
    "3. **Concatenated Output Shape**:\n",
    "\n",
    "   \\[\n",
    "   (\\text{batch\\_size}, \\text{seq\\_length}, 512)\n",
    "   \\]\n",
    "\n",
    "## Summary\n",
    "\n",
    "- **Initial Shape** of \\( Q \\), \\( K \\), and \\( V \\): \\( (\\text{batch\\_size}, \\text{seq\\_length}, \\text{embed\\_dim}) \\)\n",
    "- **After Splitting into Heads**: \\( (\\text{batch\\_size}, \\text{num\\_heads}, \\text{seq\\_length}, \\text{head\\_dim}) \\)\n",
    "- **Final Concatenated Shape**: \\( (\\text{batch\\_size}, \\text{seq\\_length}, \\text{embed\\_dim}) \\)\n",
    "\n",
    "This process ensures the attention mechanism can focus on different parts of the input sequence in parallel while maintaining the input-output dimensional consistency.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **```torch.view() : Shares the memory with original tensor```**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.5675, 0.4145, 0.7603],\n",
       "         [0.5765, 0.1945, 0.4622],\n",
       "         [0.5675, 0.8992, 0.1230],\n",
       "         [0.6485, 0.3061, 0.1271]]),\n",
       " torch.Size([4, 3]))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.rand(4,3)\n",
    "a, a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: torch.Size([2, 3])\n",
      "Reshaped shape: torch.Size([3, 2])\n",
      "tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Original tensor of shape (2, 3)\n",
    "tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(\"Original shape:\", tensor.shape)\n",
    "\n",
    "# Reshape to (3, 2)\n",
    "reshaped_tensor = tensor.view(3, 2)\n",
    "print(\"Reshaped shape:\", reshaped_tensor.shape)\n",
    "print(reshaped_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-1: When you use -1 as a dimension in the .view() method, it tells PyTorch to infer this dimension based on the size of the original tensor. This is useful when you don't know or don't want to specify the exact size of one dimension but want to ensure the total number of elements remains the same.\n",
    "\n",
    "2: This specifies that the new shape should have 2 elements in this dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix : tensor([[1, 2, 3],\n",
      "        [4, 5, 6]]) \n",
      "\n",
      "Inferred shape: torch.Size([3, 2]) \n",
      "\n",
      "Viewed output:  tensor([[1, 2],\n",
      "        [3, 4],\n",
      "        [5, 6]])\n"
     ]
    }
   ],
   "source": [
    "# Original tensor of shape (2, 3)\n",
    "x = torch.tensor([[1, 2, 3], \n",
    "                  [4, 5, 6]])\n",
    "print( \"Original matrix :\" , a, \"\\n\")\n",
    "\n",
    "# Reshape using -1\n",
    "z = x.view(-1, 2)\n",
    "print(\"Inferred shape:\", z.shape, \"\\n\")\n",
    "print(\"Viewed output: \", z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[88,  2],\n",
       "         [88,  4],\n",
       "         [88,  6]]),\n",
       " tensor([[88,  2, 88],\n",
       "         [ 4, 88,  6]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# chaning z changes x (Because View of the tensor shares the same memory as the original input)\n",
    "z[ :, 0] = 88\n",
    "z,x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1],\n",
       "        [2],\n",
       "        [3],\n",
       "        [4],\n",
       "        [5],\n",
       "        [6]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor.view(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-c6deca1ba146>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscaled_dot_product_attention\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: module 'torch.nn.functional' has no attribute 'scaled_dot_product_attention'"
     ]
    }
   ],
   "source": [
    "F.scaled_dot_product_attention()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
